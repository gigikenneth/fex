{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import DataLoader\n\n# Define your linear evaluation model architecture\nclass LinearEvaluationModel(nn.Module):\n    def __init__(self, num_classes):\n        super(LinearEvaluationModel, self).__init__()\n        # Define your linear evaluation model layers here\n        self.fc = nn.Linear(18 * 10, num_classes)\n\n    def forward(self, x):\n        # Define the forward pass of your linear evaluation model here\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Load the learned representations from the self-supervised model\nrepresentations = torch.load(\"../input/representations/representations.pth\")  # Load the representations tensor\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define hyperparameters for linear evaluation\nnum_epochs = 10\nbatch_size = 18  # Adjust the batch size to match the representations_batch size\nlearning_rate = 0.001\ngamma = 0.1  # Learning rate decay factor\nstep_size = 5  # Number of epochs after which to decay learning rate\n\n# Define your data transformations for linear evaluation\ntransform = transforms.Compose([\n    transforms.Resize((100, 100)),  # Resize input images to match the representation size\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image tensors\n])\n\n# Set the path to your labeled dataset for facial expression recognition\nlabeled_dataset_dir = \"../input/face-expression-recognition-dataset/\"\n\n# Create the labeled dataset\nlabeled_dataset = ImageFolder(labeled_dataset_dir, transform=transform)\n\n# Adjust the batch size of the labeled dataset to match the representations batch size\nlabeled_dataloader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n\n# Create the linear evaluation model\nlinear_model = LinearEvaluationModel(num_classes=len(labeled_dataset.classes)).to(device)\n\n# Define your loss function for linear evaluation\ncriterion = nn.CrossEntropyLoss()\n\n# Define your optimizer for linear evaluation\noptimizer = optim.Adam(linear_model.parameters(), lr=learning_rate)\n\n# Define your learning rate scheduler for linear evaluation\nscheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n# Training loop for linear evaluation\nfor epoch in range(num_epochs):\n    linear_model.train()  # Set the linear evaluation model to training mode\n    for batch_idx, (images, labels) in enumerate(labeled_dataloader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        representations_batch = representations[batch_idx * batch_size : (batch_idx + 1) * batch_size].to(device)\n        outputs = linear_model(representations_batch)\n\n        # Compute the linear evaluation loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Print the loss after every few iterations\n        if (batch_idx+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(labeled_dataloader)}], Loss: {loss.item():.4f}')\n\n    # Update the learning rate scheduler\n    scheduler.step()\n\n    # Display epoch information\n    print(f\"End of Epoch {epoch+1}. Learning Rate: {scheduler.get_last_lr()[0]}\")\n\n# End of linear evaluation\nprint(\"Linear evaluation finished.\")\n\n# You can now use the linear_model for facial expression recognition on new data\n# Evaluate the accuracy on a separate test set or perform predictions on new images\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-22T22:59:00.834199Z","iopub.execute_input":"2023-06-22T22:59:00.834670Z","iopub.status.idle":"2023-06-22T22:59:12.630837Z","shell.execute_reply.started":"2023-06-22T22:59:00.834638Z","shell.execute_reply":"2023-06-22T22:59:12.629138Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     74\u001b[0m representations_batch \u001b[38;5;241m=\u001b[39m representations[batch_idx \u001b[38;5;241m*\u001b[39m batch_size : (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 75\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentations_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Compute the linear evaluation loss\u001b[39;00m\n\u001b[1;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mLinearEvaluationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Define the forward pass of your linear evaluation model here\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (18x10 and 180x1)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (18x10 and 180x1)","output_type":"error"}]}]}